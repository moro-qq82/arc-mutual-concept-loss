

created by gemini 2.5 pro

主な変更点は以下の通り:
1.  **データセットの変更**：既知のプリミティブ合成タスクから、ARC-AGI-2（K-shotタスク解決）に変更。
2.  **モデルアーキテクチャの変更**：単純なEncoder-Decoderから、K-shotの例題（コンテキスト）を処理できるMeta-Learning/In-Context Learning型アーキテクチャ（例：Transformerベース）に変更。
3.  **損失関数の変更**：
    * `L_prim`（プリミティブ分類損失）は、ARCにはラベルがないため**削除**。
    * `L_share`（サブスペース共有正則化）の**グルーピング方法を変更**。プリミティブラベルの代わりに、**疎AE（SAE）によって発見された「潜在的プリミティブ（疎特徴z）」の活性化パターン**に基づいてタスクをグループ化する。
4.  **評価の変更**：「未見の合成（長3）」から「未見のARCタスク」の正解率に変更。

---

# 実験計画書：共有サブスペースXに基づくARC-AGI-2タスク一般化の検証（修正案）

## 0. 要旨（アブストラクト）
本計画は、表現空間にタスク横断で再利用可能な**共有サブスペースX**を形成させる軽量手法（サブスペース共有正則化＋疎オートエンコーダ補助）により、
(1) **ARC-AGI-2の未見タスク**への一般化（In-Context Learning性能）、(2) （メタ学習的な）few-shot適応、(3) 学習時間短縮が同時に改善されることを、単一GPU・ミニ実装で検証する。

---

## 1. 背景と問題意識
- 現行のAIは大規模事前学習により高性能だが、**抽象的推論**や**極端な少データ一般化**（例：ARCタスク）に課題が残る。
- 人間の推論は、少数例から**潜在的な「ルール」や「操作プリミティブ」**を抽出し、それを再利用している。
- 我々の作業仮説：モデル表現（特にタスクの「解法ルール」を捉えるベクトル）に**共有サブスペースX**を明示的に持たせ、その再利用を促すと、少ない例から新規タスクに効率よく適応できる。

---

## 2. 目的（検証したい主張）
1.  **共有サブスペースX形成**（潜在的ルールの部品化）を促す軽量な学習目標で、未見のARCタスクに対する**In-Context Learning性能（K-shot正解率）**が改善する。
2.  上記効果は、**学習時間（到達ステップ/壁時計時間）**の観点でも有意。
3.  表現解析（CKA/Grassmann距離・疎特徴の可視化）により、**Xの存在と再利用**（例：類似の操作を行うタスク群が共通のサブスペースや疎特徴を共有）を定量・定性に確認できる。

---

## 3. 実験デザインの概要
- **データ**：**ARC-AGI-2データセット**。
  - 各サンプルは1つの「タスク」であり、K個（例：3〜5個）の「学習例ペア（入力グリッド、出力グリッド）」と1個の「テスト入力グリッド」から成る。
  - 目標は「テスト出力グリッド」を正確に予測すること。
- **モデル**：**K-shot In-Context Learningモデル**（例：小型Transformer）。
  - **Grid Encoder** (例: CNN) が各グリッドをベクトル化。
  - **Context Encoder** (例: Transformer) がK個の学習例ペア `(in_1, out_1), ..., (in_K, out_K)` を処理し、**タスク表現 $h_{task}$** を抽出する。
  - **Solver** (例: Transformer Decoder) が `(test_in, h_{task})` を条件として `test_out` を予測（回帰またはトークンごとの分類）。
  - **疎AE**：**$h_{task}$** → $z$（疎）→ $\hat{h}_{task}$ の再構成で“解法ルールの部品化”。
- **鍵となる損失**：
  - **サブスペース共有正則化**：
    - SAEの中間特徴 $z$ の**活性化パターン**に基づき、バッチ内のタスクを**動的にグループ化**（例：特徴 $z_j$ が活性化したタスク群 $G_j$）。
    - $G_j$ に属する $h_{task}$ の表現行列の上位k主成分から射影行列 $P_j$ を作り、グループ群の平均射影 $\bar P$ との二乗距離を最小化。
  - **疎AE補助**（L1による疎性＋再構成MSE）。
  - **タスク損失**（テスト出力グリッドのCE / ピクセル単位Acc）。
- **（削除）**：プリミティブ多ラベルBCE（ARCにはラベルがないため）。

---

## 4. 仮説と評価指標
- **H1（タスク一般化）**：提案法はB0（ベースライン）より、未見ARCタスク（テストセット）での**タスク正解率（Top-1 / Top-3）**または**ピクセルAcc/IoU**が有意に高い。
- **H2（少データ適応）**：[変更] 未見タスク群（例：100タスク）への**メタ適応**（追加ファインチューニング）において、**タスク数–性能**曲線・**到達ステップ**が優位。
- **H3（学習時間効率）**：val-Acc到達までの**学習ステップ/時間**を短縮。
- **H4（表現の共有）**：**SAE特徴 $z_j$ によって定義されたタスク群**間の**CKA/Grassmann距離**が小さく、共有度が高い。疎特徴 $z$ がARCの（人間が解釈可能な）**潜在的操作**（例：「色の置換」「図形のコピー」）と有意に対応。

**主要指標**：
- 一般化：未見ARCタスクの正解率（K-shot In-Context Learning）。
- 適応効率：メタ適応時の到達ステップ、到達時間、必要タスク数。
- 共有度：SAE特徴グループ間のCKA/Grassmann距離、線形プローブ転移精度（*もし何らかのメタラベルがあれば*）。
- 解釈：$z$ ユニット活性とタスクタイプの相関、可視化例（$z_j$ が活性化したタスク群の例）。

---

## 5. ベースラインとアブレーション
- **B0**：標準学習（共有正則化・疎AEなし）。
- **B1**：補助のみ（疎AE、共有正則化なし）。
- **B2**：共有のみ（共有正則化、疎AEなし）。
  - [注] この場合、グルーピングは**$h_{task}$ のk-meansクラスタリング**などで代替する必要がある。
- **B3**：疎AEの**非疎**（L1=0）。
- **B4**：few-shot（メタ適応）で**Adapter/LoRAなし**の全層微調整。
- **B5**：**Context Encoder**のアーキテクチャ比較（例：Transformer vs Deep Sets vs GRU）。
- **B6**：サブスペース次元kのスイープ（8/16/32）。

---

## 6. 手順（プロトコル）
1.  **データ準備**：ARC-AGI-2データローダー準備（学習/検証/テスト）。
2.  **事前学習**：ARC学習セットで学習。損失 $\mathcal{L}=\mathcal{L}_{task}+\alpha\mathcal{L}_{share}+\beta\mathcal{L}_{sae}$。初期設定：$\alpha=0.1, \beta=0.1$。
3.  **In-Context Learning評価**：未見タスク（テストセット）で性能測定（**勾配更新なし**）。
4.  **few-shotメタ適応**：[変更] 未見タスク群（例：50〜100タスク）で**LoRA/AdapterをContext Encoder近傍に限定**し微調整（≤500 step）。
5.  **学習時間記録**：同一GPUで学習曲線（step vs val-Acc）と壁時計時間をログ。
6.  **表現解析**：
    - SAE特徴 $z_j$ でグループ化したタスク群間のCKA/Grassmann距離。
    - SAEの $z$ とタスクの（人間による）カテゴリ分類との相関・活性ヒートマップ。

---

## 7. 実装上の簡素化と安定化策
- **PCA**：`torch.pca_lowrank`で小規模バッチから上位k次元。平均射影は**detach**、1–Nステップごと更新。
- **グループ分け**：[変更] バッチ内で**SAE特徴 $z_j$ が閾値以上で活性化したタスク**をグループ $G_j$ とする。バッチ内に最低（例：16）サンプル以上存在する $j$ のみで $\mathcal{L}_{share}$ を計算。
- **疎化**：L1はウォームアップで増加。過疎/過密を監視し $\lambda$ を自動調整（目標平均活性率）。
- **過度共有の回避**：$\alpha$ を0.05–0.2でスイープ。早期停止を併用。

---

## 8. 計算資源とタイムライン
- **資源**：単一GPU（**3090/A100/H100推奨**。ARCはT4/V100では厳しい可能性）。
- **所要**：[変更] 学習**12–48時間**、適応各数分〜十数分（環境依存）。
- **タイムライン**：
  - Week 1：データローダー・ベースラインモデル（B0）実装、初期学習。
  - Week 2：共有正則化・疎AE追加、In-Context評価。
  - Week 3：メタ適応、表現解析。
  - Week 4：アブレーション、図表作成・短報ドラフト。

---

## 9. 期待される結果と意義
- 共有サブスペース正則化＋疎AEにより、**未見ARCタスクの正解率**が上昇し、**到達時間**が短縮。
- 解析により、**SAE特徴グループ間のサブスペースの収束**と**疎特徴の（解釈可能な）潜在的ルール対応**を確認。
- ARCタスクにおける**抽象的ルールの部品化と再利用**の実現に向けた、シンプルなベースラインを提供。

---

## 10. リスクと代替案
- **PCA不安定**：グループサイズ拡大、EMA平均。
- **表現崩壊**：$\alpha$ を縮小、補助損失の重み調整。
- **[変更] タスクが困難すぎる**：ベースライン（B0）の正解率が0%に近い場合、改善が観測困難。→ **ARCの簡易版**（例：色のみ、形状のみのタスク）や、より解きやすい**1D-ARC**等でまず手法の有効性を検証する。
- **[新規] SAEが自明な特徴を獲得**：$\lambda$ の調整、SAEのアーキテクチャ見直し。

---

## 11. 公開物と再現性
- コード（ARCローダー＋モデル＋損失関数）。
- ログとシード、ハイパラ表、チェックポイント。
- 図：学習曲線、タスク正解率バー、CKA/Grassmannヒートマップ、**SAE特徴 $z_j$ が捉えたタスク群の可視化例**。

---

## 12. 短報ドラフト構成（目安）
(変更なし)

---

## 付録A：損失の定式化（簡易）
- タスク損失：$\mathcal{L}_{task}=\text{CE}(\hat{Y}_{test}, Y_{test})$。
- 疎AE：$h = h_{task}$。$\mathcal{L}_{sae}=\|h-\hat h\|_2^2+\lambda\|z\|_1$。
- 共有正則化：[変更] グループ $G_j$ = $\{h_i \mid z_i[j] > \theta\}$（バッチ $i$ 内で $j$ が活性化したタスク）。
  - $H_j$ (中心化表現行列) から $U_j=\mathrm{PCA}_k(H_j)$、\(P_j=U_jU_j^T\)。
  - $\bar P=\frac{1}{|J|}\sum_{j \in J} P_j$ ($J$ は有効な特徴インデックス集合)。
  \[\mathcal{L}_{share}=\frac{1}{|J|}\sum_{j \in J} \|P_j-\bar P\|_F^2\]
- **（削除）**：$\mathcal{L}_{prim}$。
- 総損失：$\mathcal{L}=\mathcal{L}_{task}+\alpha\mathcal{L}_{share}+\beta\mathcal{L}_{sae}$。

---

## 付録B：few-shotメタ適応の範囲
- LoRA/Adapterは**Context Encoder**または**Solver**のTransformer層に限定。学習率小、step ≤ 500。
- 早期停止基準：メタ適応用の検証タスク群での正解率改善停滞。